## LLM Benchmark Visualization
A Python-based visualization tool for benchmarking large language models (LLMs) using ELO scores. This project generates bar charts with confidence intervals to compare models across multiple evaluation metrics, such as Freshness, Factuality, Helpfulness, and Holistic performance.

**Features:**
- Supports multiple LLMs (e.g., pplx-7b, pplx-70b, llama2-70b, gpt-3.5)
- Generates **bar charts with confidence intervals** using matplotlib
- Customizable color schemes for different evaluation categories
- Easily extendable for other benchmark datasets

**Tech Stack:**
- Python (matplotlib, numpy, pandas)
- Data visualization for AI/ML model evaluation
